---
title: "Hard Work 3 Part 2"
output: 
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
    code_folding: hide
---
```{r, include=FALSE}
library(tidyverse)
library(broom)
library(car)
library(alr3)
library(qqplotr)
library(patchwork)
```

## Instructions

1. Study Sections 3.9-3.11 of Chapter 3: "Diagnostics and Remedial Measures."    

2. Attempt and submit at least <span id=points style="padding-left:0px;">{27}</span> Hard Work Points by Saturday at 11:59 PM.    
<span id=note>Over <span id=points style="padding-left:0px;">{37}</span> gets you {+1} Final Exam Point.</span>    

## Reading Points <span id=headpoints>{18} Possible</span>

### Section 3.9 <span id=recpoints>{7}</span><span id=report>{ 7 / 7 }</span>

### Section 3.10 <span id=recpoints>{4}</span><span id=report>{ 4 / 4 }</span>

### Section 3.11 <span id=recpoints>{7}</span><span id=report>{ 7 / 7 }</span>



## Theory Points <span id=headpoints>{7} Possible</span>



### Exercise 3.20 <span id=recpoints>{3}</span><span id=report>{ 3 / 3 }</span> 

If the error terms in a regression model are independent $N(0,\sigma^{2})$, what can be said about the error terms after transformation $X^{\prime} = 1/X$ is used? Is the situation the same after transformation $Y^{\prime} = 1/Y$ is used?

__Solution:__
<div id="rcorners2">
The error terms would not change after a transformation on the X variable because transformations on the X variable are meant to fix linearity problems while keeping variance constant. While the transformation on the Y variable is meant to fix constant variance problems. </div></br>
 
### Exercise 3.22 <span id=points>{4}</span><span id=report>{ 4 / 4 }</span> 

Using (A.70), (A.41), and (A.42), show that $E[MSPE] = \sigma^{2}$ for normal error regression model (2.1).

__Solution:__
<div id="rcorners3">
\begin{align*}
E[MSPE] &= E\left[\dfrac{\Sigma\Sigma\left(Y_{ij}-\bar{Y_{j}}\right)^{2}}{n-c}\right] \\
&= \dfrac{1}{n-c}\Sigma E[(n_{j} - 1)s^{2}_{j}] \\
&= \dfrac{1}{n-c}\Sigma E[\sigma^{2}\chi^{2}(n_{j}-1)] \\
&= \dfrac{\sigma^{2}}{n-c}\Sigma(n_{j}-1) \\
&= \sigma^{2}
\end{align*}
</div></br>


## Application Points <span id=headpoints>{17} Possible</span>

<a id=datalink style="font-size:.9em;" target="_blank" href=http://www.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/index.html>Data Files</a>


### Problem 3.16 <span id=recpoints>{6}</span><span id=report>{ 6 / 6 }</span>

```{r p316}
# Load the Data:
p3.15 <- read.table("http://www.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%203%20Data%20Sets/CH03PR15.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p3.15) <- c("Y","X")

sol.lm <- lm(Y ~ X, p3.15)

scatp <- augment(sol.lm) %>% 
  ggplot(aes(x = X, y = Y)) + 
  geom_point(shape = 1) + 
  labs(title = "Scatter Plot of Data Values") +
  theme_bw()
```

__Refer to Solution Concentration Problem 3.15__

  __a)__ Prepare a scatter plot of the data. What transformation of Y might you try, using the prototype patterns in Figure 3.15 to achieve constant variance and linearity?
  
  __Solution:__
  <div id="rcorners2">
```{r, fig.align='center'}
scatp
```
  It looks like there is a linearity issue and variance issue. So I would probably do a transformation on both the Y and X variables. For $Y$, I would use $Y^{\prime} = \log_{10} Y$. And for $X$ I would use $X^{\prime} = 1/x.$
  </div></br>
  
  __b)__ Use the Box-Cox procedure and standardization (3.36) to find an appropriate power transformation. Evaluate SSE for $\lambda = -.2,-.1,0,.1,.2$. What transformation of Y is suggested?
  
  __Solution:__
  <div id="rcorners2">
```{r,fig.align='center'}
boxCox(sol.lm, lambda = seq(-.2,.2,.1))
```
  It looks like the suggested $\lambda$ value is anywhere from -0.5 to 1.2
  </div></br>
  
  __c)__ Use the transformation $Y^{\prime} = \log_{10} Y$ and obtain the estimated linear regression function for the transformed data.
  
  __Solution:__
  <div id="rcorners2">
```{r}
sol.log.lm <- lm(log10(Y) ~ X, p3.15) 
summary(sol.log.lm)
```
\[
\log_{10}{Y_{i}} = \hat{Y_{i}}^{\prime} = b_{0} + b_{1}X_{i} \\
\log_{10}{Y_{i}} = \hat{Y_{i}}^{\prime} = `r tidy(sol.log.lm)[1,2]` + `r tidy(sol.log.lm)[2,2]`X_{i}
\]
  </div></br>
  
  __d)__ Plot the estimated regression line and the transformed data. Does the regression line appear to be a good fit to the transformed data?
  
  __Solution:__
  <div id="rcorners2">
```{r, fig.align='center'}
augment(sol.log.lm) %>% 
  ggplot(aes(x = X, y = log10.Y.)) + 
  geom_point() + 
  geom_smooth(method="lm", se = FALSE, color = "darkred") + 
  theme_bw()
```
  It looks better but there are still some variance issues.
  </div></br>
  
  __e)__ Obtain the residuals and plot them against the fitted values. Also prepare a normal probability plot. What do your plots show?
  
  __Solution:__
  <div id="rcorners2">
```{r, fig.align='center'}
fitted.resid <- augment(sol.log.lm) %>% 
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_point(shape = 1) + 
  geom_smooth(method="loess", se=FALSE, color="red", size = .5) +
  labs(title = "Residuals Against Fitted Vaues",
       x = "Fitted Values \n LM(Log10(Y) ~ X)", y = "Residuals") +
  theme_bw() 

qq.norm <- augment(sol.log.lm) %>% 
  ggplot(aes(sample = .resid)) + 
  stat_qq_band(color = "black", fill = NA, linetype = 2) +
  stat_qq_line(color = "red", size = .5) +
  stat_qq_point(shape = 1) +
  labs(title = "Normal Q-Q",
       x = "Theoretical Quantiles \n lm(Y ~ X)", 
       y = "Standardized residuals") +
  theme_bw()

fitted.resid + qq.norm
```
  The errors appear to be normally distributed but the residuals against the fitted values are showing massive linearity issues and variance problems. 
  </div></br>
  
  __f)__ Express the estimated regression function in the original units. 
  
  __Solution:__
  <div id="rcorners2">
\[
\hat{Y_{i}} = 10^{(b_{0}+b_{1}X_{i})} \\
\hat{Y_{i}} = 10^{(`r tidy(sol.log.lm)[1,2]` + `r tidy(sol.log.lm)[2,2]`X_{i})}
\]
  </div></br>

### Problem 3.17 <span id=points>{6}</span><span id=report>{ 6 / 6 }</span>

```{r p317}
# Load the Data:
p3.17 <- read.table("http://www.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%203%20Data%20Sets/CH03PR17.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p3.17) <- c("Y","X")

sales.lm <- lm(Y ~ X, p3.17)

scatp <- augment(sales.lm) %>% 
  ggplot(aes(x = X, y = Y)) +
  geom_point(shape = 1) +
  labs(title = "Scatter Plot of Data") + 
  theme_bw()
```

__Sales Growth. A marketing researcher studied annual sales of a product that had been introduced 10 years ago. The data are as follows, where X is the year (coded) and Y is sales in thousands.__

  __a)__ Prepare a scatter plot of the data. Does a linear relation appear adequate here?
  
  __Solution:__
  <div id="rcorners3">
```{r, fig.align='center'}
scatp
```
  Yes, these data appear to have a linear relationship. 
  </div></br>
  
  __b)__ Use the Box-Cox procedure and standardization (3.36) to find an appropriate power transformation of Y. Evaluate SSE for $\lambda = .3,.4,.5,.6,.7$. What transformation of Y is suggested?
  
  __Solution:__
  <div id="rcorners3">
```{r, fig.align='center'}
boxCox(sales.lm, lambda = seq(.3,.7,.1))
```
  It would probably be best to assign $\lambda = 0.5$
  </div></br>
  
  __c)__ Use the transformation $Y^{\prime} = \sqrt{Y}$ and obtain the estimated linear regression function for the transformed data. 
  
  __Solution:__
  <div id="rcorners3">
```{r}
sales.sqrt.lm <- lm(sqrt(Y) ~ X, p3.17)
summary(sales.sqrt.lm)
```
\[
\sqrt{Y_{i}} = \hat{Y_{i}}^{\prime} = b_0 + b_1X_i \\
\sqrt{Y_{i}} = \hat{Y_{i}}^{\prime} = `r tidy(sales.sqrt.lm)[1,2]` + `r tidy(sales.sqrt.lm)[2,2]`X_i
\]
  </div></br>
  
  __d)__ Plot the estimated regression line and the transformed data. Does the regression line appear to be a good fit to the transformed data?
  
  __Solution:__
  <div id="rcorners3">
```{r, fig.align='center'}
augment(sales.sqrt.lm) %>% 
  ggplot(aes(x = X, y = sqrt.Y.)) + 
  geom_point() + 
  geom_smooth(method="lm", se = FALSE, color = "darkred") + 
  theme_bw()
```
Yes, it appears to fit well to these data. 
  </div></br>
  
  __e)__ Obtain the the residuals plot and plot them against the fitted values. Also prepare a normal probability plot. What do your plots show?
  
  __Solution:__
  <div id="rcorners3">
```{r, fig.align='center'}
fitted.resid <- augment(sales.sqrt.lm) %>% 
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_point(shape = 1) + 
  geom_smooth(method="loess", se=FALSE, color="red", size = .5) +
  labs(title = "Residuals Against Fitted Vaues",
       x = "Fitted Values \n LM(Log10(Y) ~ X)", y = "Residuals") +
  theme_bw() 

qq.norm <- augment(sales.sqrt.lm) %>% 
  ggplot(aes(sample = .resid)) + 
  stat_qq_band(color = "black", fill = NA, linetype = 2) +
  stat_qq_line(color = "red", size = .5) +
  stat_qq_point(shape = 1) +
  labs(title = "Normal Q-Q",
       x = "Theoretical Quantiles \n lm(Y ~ X)", 
       y = "Standardized residuals") +
  theme_bw()

fitted.resid + qq.norm
```
It shows that the residuals exhibit non-linearity and non-constant variance but the residuals are normally distributed. 
  </div></br>
  
  __f)__ Express the estimated regression function in the original units. 
  
  __Solution:__
  <div id="rcorners3">
\[
\hat{Y_{i}} = (`r tidy(sales.sqrt.lm)[1,2]` + `r tidy(sales.sqrt.lm)[2,2]`X_i)^{2}
\]
  </div></br>

### Problem 3.18 <span id=points>{5}</span><span id=report>{ 5 / 5 }</span>

```{r p318}
# Load the Data:
p3.18 <- read.table("http://www.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%203%20Data%20Sets/CH03PR18.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p3.18) <- c("Y","X")

prod.lm <- lm(Y ~ X, p3.18)

scatp <- augment(prod.lm) %>% 
  ggplot(aes(x = X, y = Y)) + 
  geom_point(shape = 1) +
  labs(title = "Scatter Plot of Data") + 
  theme_bw()
```

__Production time. In a manufacturing study, the production times for 111 recent production runs were obtained. The data contains the production time in hours (Y) and the production lot size (X).__

  __a)__ Prepare a scatter plot of the data. Does a linear relation appear adequate here? Would a transformation on X or Y be more appropriate here? Why?
  
  __Solution:__
  <div id="rcorners3">
```{r, fig.align='center'}
scatp
```
I would use the transformation $X^{\prime} = \sqrt{X}$
  </div></br>
  
  __b)__ Use the transformation $X^{\prime} = \sqrt{X}$ and obtain the estimated linear regression function for the transformed data. 
  
  __Solution:__
  <div id="rcorners3">
```{r}
prod.sqrt.lm <- lm(Y ~ I(sqrt(X)), p3.18)
summary(prod.sqrt.lm)
```
\[
\hat{Y_{i}} = `r tidy(prod.sqrt.lm)[1,2]` + `r tidy(prod.sqrt.lm)[2,2]`\sqrt{X_{i}}
\]
  </div></br>
  
  __c)__ Plot the estimated regression line and the transformed data. Does the regression line appear to be a good fit to the transformed data?
  
  __Solution:__
  <div id="rcorners3">
```{r}
augment(prod.sqrt.lm) %>% 
  ggplot(aes(x = I.sqrt.X.., y = Y)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  theme_bw()
```
  Yes it looks like a good fit. 
  </div></br>
  
  __d)__ Obtain the residuals and plot them against the fitted values. Also prepare a normal probability plot. What do your plots show?
  
  __Solution:__
  <div id="rcorners3">
```{r}
fitted.resid <- augment(prod.sqrt.lm) %>% 
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_point(shape = 1) + 
  geom_smooth(method="loess", se=FALSE, color="red", size = .5) +
  labs(title = "Residuals Against Fitted Vaues",
       x = "Fitted Values \n LM(Log10(Y) ~ X)", y = "Residuals") +
  theme_bw() 

qq.norm <- augment(prod.sqrt.lm) %>% 
  ggplot(aes(sample = .resid)) + 
  stat_qq_band(color = "black", fill = NA, linetype = 2) +
  stat_qq_line(color = "red", size = .5) +
  stat_qq_point(shape = 1) +
  labs(title = "Normal Q-Q",
       x = "Theoretical Quantiles \n lm(Y ~ X)", 
       y = "Standardized residuals") +
  theme_bw()

fitted.resid + qq.norm
```

  </div></br>
  
  __e)__ Express the estimated regression function in the original units. 
  
  __Solution:__
  <div id="rcorners3">
  \[
  \hat{Y_{i}}^{2} = `r tidy(prod.sqrt.lm)[1,2]` + `r tidy(prod.sqrt.lm)[2,2]`X_{i}
  \]
  </div></br>


 <style>
#points {
  font-size:.8em;
  padding-left:5px;
  font-weight:bold; 
  color:#317eac;
}

#recpoints {
  font-size:.8em;
  padding-left:5px;
  font-weight:bold; 
  color:#7eac31;
}

#report {
  font-size:.7em;
  padding-left:15px;
  font-weight:normal; 
  color:#5a5a5a;
}

#datalink {
  font-size:.5em;
  color:#317eac;
  padding-left:5px;
}

#headnote {
  font-size:.6em;
  color:#787878;
}

#note {
  font-size:.8em;
  color:#787878;
}

#headpoints {
 font-size:12pt;
 color: #585858; 
 padding-left: 15px;
}

#rcorners2 {
    border-radius: 25px;
    border: 2px solid #73AD21;
    padding: 20px; 
    width: auto;
    height: auto;    
}

#rcorners3 {
    border-radius: 25px;
    border: 2px solid #317eac;
    padding: 20px; 
    width: auto;
    height: auto;    
}
</style>


<footer>
</footer>



 

 

 

 