---
title: "Hard Work 11"
output: 
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
    code_folding: hide
---
```{r, include=FALSE}
library(tidyverse)
library(broom)
library(patchwork)
```

<style>

</style>

## Instructions

1. Study Sections 11.4 (first) and then 11.3 -- "Regression Trees and Robust Regression."

2. Attempt and submit at least <span id=points style="padding-left:0px;">{44}</span> Hard Work Points by Saturday at 11:59 PM.    
<span id=note>Over <span id=points style="padding-left:0px;">{47}</span> gets you {+1} Final Exam Point.</span>    


## Reading Points <span id=headpoints>{19} Possible</span>

### Section 11.4 <span id=rrecpoints>{7}</span><span id=report>{ 7 / 7 }</span>


### Section 11.3 <span id=rrecpoints>{12}</span><span id=report>{ 12 / 12 }</span>




## Theory Points <span id=headpoints>{4} Possible</span>

### Problem 11.2 <span id=points>{2}</span><span id=report>{ 2 / 2 }</span>

An analyst suggested: "One nice thing about robust regression is that you need not worry about outliers and influential observations." Comment. 

__Solution:__
<div id="rcorners3">
I would say that is a misguided comment. While a robust regression can have dampening effects that outliers may have on the predicted regression line, that is not to say that it is okay to throw caution to the wind and never have to worry about outliers. Outliers should only be taken out in special circumstances and with extreme caution. 
</div></br>

 
### Problem 11.4 <span id=points>{2}</span><span id=report>{ 2 / 2 }</span>

Regression trees become difficult to utilize when there are many predictors and the sample size is small. Discuss the nature of this problem.

__Solution:__
<div id="rcorners3">
The one good thing about regression trees is that they are the closest thing to machine leanring that has an interpretable output. But once you have too many predictors and not enough sample size you start to deal with overfitting of your data. 
</div></br>

## Application Points <span id=headpoints>{25} Possible</span>


<a id=datalink style="font-size:.9em;" target="_blank" href=http://www.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/index.html>Data Files</a>

### Problem CO2 <span id=recpoints>{10}</span><span id=report>{ / }</span>

Create a graphic similar to Figure 11.9 of the textbook for the CO2 data. 
Interpret your graphic.

```{r, include=FALSE}
# Install the party library
# Run once: install.packages("party")
library(party)
```

```{r, eval=TRUE, fig.align='center', fig.width=6, fig.height=4}
# Set eval=TRUE to run this code.
ctree <- ctree(uptake ~ conc + Treatment + Type, data=CO2)
plot(ctree, type="simple")

pal <- c("#FBB4AE", "#B3CDE3", "#CCEBC5", "#DECBE4")

ggplot(CO2, aes(conc, uptake, color = interaction(Type, Treatment))) + 
  geom_point() + 
  geom_segment(aes(x = 95, xend = 175, y = 20.575, yend = 20.575), color = pal[1]) +
  geom_segment(aes(x = 176, xend = 500, y = 37.38, yend = 37.38), color = pal[1]) +
  geom_segment(aes(x = 500, xend = 1000, y = 40.75, yend = 40.75), color = pal[1]) +
  geom_segment(aes(x = 95, xend = 250, y = 19.68, yend = 19.68), color = pal[2]) +
  geom_segment(aes(x = 250, xend = 1000, y = 30.66, yend = 30.66), color = pal[2]) +
  geom_segment(aes(x = 95, xend = 250, y = 13.49, yend = 13.49), color = pal[4]) +
  geom_segment(aes(x = 250, xend = 1000, y = 17.56, yend = 17.56), color = pal[4]) +
  labs(title = "Regression Tree Plot", x = "Concentration", y = "C02 Uptake") +
  scale_color_manual("Type*Treatment", 
                     values = c("#FBB4AE", "#B3CDE3", "#CCEBC5", "#DECBE4")) +
  theme_classic() 
```


### Problem 11.11 <span id=recpoints>{7}</span><span id=report>{ 7 / 7 }</span> 

```{r}
# Code to read in the data
# It has to be merged from the 1.20 and 8.15 data files.
p11.11 <- read.table("http://www.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%2011%20Data%20Sets/CH11PR11.txt", header=FALSE)

p1.20 <- read.table("http://www.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%201%20Data%20Sets/CH01PR20.txt", header=FALSE)

p11.11 <- rbind(p1.20, p11.11)

# Give it nice column names for conevience:
colnames(p11.11) <- c("Y","X1")

# Show the data:
# p11.11
```

Refer to the __Copier Mantenance Problem 1.20.__ Two cases had been held out of the original data set because special circumstances led to unusually long service times:

<center>
```{r, fig.align='center'}
tibble(i = c(46,47), X = c(6,5), Y = c(132,166)) %>% 
  pander::pander()
```
</center>

  __a)__ Using the enlarged (47-case) data set, fit a simple linear regression model using ordinary least squares and plot the data together with the fitted regression function. What is the effect of adding cases 46 and 47 on the fitted response function?
  
  __Solution:__
  <div id="rcorners2">
```{r}
copy.lm <- lm(Y ~ X1, p11.11)

ggplot(p11.11, aes(X1, Y)) + 
  geom_point(shape = 1) + 
  geom_smooth(method = "lm", se = FALSE, size = .5, color = "skyblue2") + 
  labs(title = "Simple OLS Regression") + 
  theme_bw()
```
  
  </div></br>
  
  __b)__ Obtain the scaled residuals in (11.47) and use the Huber weight function (11.44) to obtain the case weights for a first iteration of IRLS robust regression. Which case receive the smallest Huber weights? Why?
  
  __Solution:__
  <div id="rcorners2">
```{r}
mad <- 1/.6745*median(abs(augment(copy.lm)$.resid - median(augment(copy.lm)$.resid)))
u <- augment(copy.lm)$.resid/mad

huber <- function(u){
  ifelse(abs(u) <= 1.345, 1, 1.345/abs(u))
}

huber(u) %>% 
  pander::pander()
```
  
  </div></br>
  
  __c)__ Using the weights calculated in part (b), obtain the weighted least squares estimates of the regression coefficients. How do the estimates compare to those found in part (a) using ordinary least squares?
  
  __Solution:__
  <div id="rcorners2">
```{r}
copy.rlm <- lm(Y ~ X1, p11.11, weights = huber(u))
summary(copy.rlm)
```
  
  </div></br>
  
  __d)__ Continue the IRLS procedure for two more iterations. Which cases receive the smallest weights in the final iteration? How do the final IRLS robust regression estimates compare to the ordinary least squares estimates obtained in part (a)?
  
  __Solution:__
  <div id="rcorners2">
```{r}
mad <- 1/.6745*median(abs(augment(copy.rlm)$.resid - median(augment(copy.rlm)$.resid)))
u <- augment(copy.rlm)$.resid/mad

copy.rlm2 <- lm(Y ~ X1, p11.11, weights = huber(u))

mad <- 1/.6745*median(abs(augment(copy.rlm2)$.resid - median(augment(copy.rlm2)$.resid)))
u <- augment(copy.rlm2)$.resid/mad

copy.rlm3 <- lm(Y ~ X1, p11.11, weights = huber(u))

huber(u) %>% 
  pander::pander(caption = "2 More Itrations of Huber Function, ending weights")

tidy(copy.rlm3) %>% 
  pander::pander(caption = "2 More Iterations of Huber Regression Output")
```
  
  </div></br>
  
  __e)__ Plot the final IRLS estimated regression function, obtained in part (d), on the graph constructed in part (a). Does the robust fit differ substantially from the ordinary least squares fit? If so, which fit is preferred here?
  
  __Solution:__
  <div id="rcorners2">
```{r, fig.align='center', fig.width=6, fig.height=4}
dat <- tibble(Y = augment(copy.lm)$Y, 
       X1 = augment(copy.lm)$X1,
       fitted.a = augment(copy.lm)$.fitted,
       fitted.d = augment(copy.rlm3)$.fitted)

ggplot() +
  geom_point(data = dat, aes(X1, Y), shape = 1) + 
  geom_line(data = dat, aes(x = X1, y = fitted.a, color = "ols")) + 
  geom_line(data = dat, aes(x = X1, y = fitted.d, color = "robust")) + 
  labs(title = "Simple OLS vs. Robust OLS") + 
  scale_color_manual("Model", values = c("ols" = "skyblue2", "robust" = "firebrick2")) +
  theme_bw()
```

  </div></br>
  
### Problem 11.12 <span id=recpoints>{8}</span><span id=report>{ 8 / 8 }</span> 

```{r}
# Code to read in the data
# It has to be merged from the 1.20 and 8.15 data files.
p11.12 <- read.table("http://www.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%2011%20Data%20Sets/CH11PR12.txt", header=FALSE)

# Give it nice column names for conevience:
colnames(p11.12) <- c("Y","X1")

# Show the data:
# p11.12
```

__Weight and Height.__ The weights and heights of twenty male students in a freshman class are recorded in order to see how well weight (Y, in pounds) can be predicted from height (X, in inches). The data are given below. Assume that first-order regression (1.1) is appropriate. 

<center>
```{r}
head(p11.12, 2) %>% 
  pander::pander()
```
</center>

  __a)__ Fit a simple linear regression model using ordinary least squares and plot the data together with the fitted regression function. Also, obtain an index plot of Cook's distance (10.33). What do these plots suggest?
  
  __Solution:__
  <div id="rcorners2">
```{r, fig.align='center', fig.width=6, fig.height=4}
weight.lm <- lm(Y ~ X1, p11.12)

ols <- ggplot(p11.12, aes(X1, Y)) + 
  geom_point(shape = 1) + 
  geom_smooth(method = "lm", se = FALSE, size = .5, color = "firebrick2") + 
  labs(title = "Simple OLS Regression") + 
  theme_bw()
  
N <- nrow(weight.lm$model) 
df <- data.frame(Index = 1:N,  Cook = cooks.distance(weight.lm))
df$group <- factor(ifelse(df$Cook > 0.01, 1, 0))

cooks.dist <- ggplot(df, aes(Index, Cook, color=group, group=group)) +
  geom_point(size=2) +
  geom_segment(aes(Index, xend=Index, 0, yend=Cook, color=group), data=df)  +
  theme_bw() +
  scale_color_manual(values=c("black", "firebrick2")) +
  ylab("Cook's Distance") +
  ggtitle("Index Plot of Cook's Distance") +
  theme(legend.position = "none")

ols + cooks.dist
```
  
  </div></br>
  
  __b)__ Obtain the scaled residuals in (11.47) and use the Huber weight function (11.44) to obtain case weights for a first iteration of IRLS robust regression. Which cases receive the smallest Huber weights? Why?
  
  __Solution:__
  <div id="rcorners2">
```{r}
mad <- 1/.6745*median(abs(augment(weight.lm)$.resid - median(augment(weight.lm)$.resid)))
u <- augment(weight.lm)$.resid/mad

huber <- function(u){
  ifelse(abs(u) <= 1.345, 1, 1.345/abs(u))
}

huber(u) %>% 
  pander::pander()
```
  </div></br>
  
  __c)__ Using the weights calculated in part (b) obtain the weighted least squares estimates of the regression coefficients. How do these estimates compare to those found in part (a) using ordinary least squares?
  
  __Solution:__
  <div id="rcorners2">
```{r}
weight.rlm <- lm(Y ~ X1, p11.12, weights = huber(u))
summary(weight.rlm)

tidy(weight.lm) %>% 
  pander::pander(caption = "Non-Robust OLS Regression weight.lm")

tidy(weight.rlm) %>% 
  pander::pander(caption = "Robust OLS Regression weight.rlm")
```
  
  </div></br>
  
  
  __d)__ Continue the IRLS procedure for two more iterations. Which cases receive the smallest weights in the final iteration? How do the final IRLS robust regression estimates compare to the ordinary least squares estimates obtained in part (a)?
  
  __Solution:__
  <div id="rcorners2">
```{r}
mad <- 1/.6745*median(abs(augment(weight.rlm)$.resid - median(augment(weight.rlm)$.resid)))
u <- augment(weight.rlm)$.resid/mad
weight.rlm2 <- lm(Y ~ X1, p11.12, weights = huber(u))
mad <- 1/.6745*median(abs(augment(weight.rlm2)$.resid - median(augment(weight.rlm2)$.resid)))
u <- augment(weight.rlm2)$.resid/mad
weight.rlm3 <- lm(Y ~ X1, p11.12, weights = huber(u))

huber(u) %>% 
  pander::pander(caption = "2 More Itrations of Huber Function, ending weights")

tidy(weight.rlm3) %>% 
  pander::pander(caption = "2 More Iterations of Huber Regression Output")
```
  
  </div></br>
  

<footer>
</footer>



 
 <style>
 #points {
  font-size:.8em;
  padding-left:5px;
  font-weight:bold; 
  color:#317eac;
}

#recpoints {
  font-size:.8em;
  padding-left:5px;
  font-weight:bold; 
  color:#7eac31;
}

#rrecpoints {
  font-size:1em;
  padding-left:5px;
  font-weight:bold; 
  color:#7eac31;
}

#report {
  font-size:.7em;
  padding-left:15px;
  font-weight:normal; 
  color:#5a5a5a;
}

#datalink {
  font-size:.5em;
  color:#317eac;
  padding-left:5px;
}

#headnote {
  font-size:.6em;
  color:#787878;
}

#note {
  font-size:.8em;
  color:#787878;
}

#headpoints {
 font-size:12pt;
 color: #585858; 
 padding-left: 15px;
}
#rcorners2 {
    border-radius: 25px;
    border: 2px solid #73AD21;
    padding: 20px; 
    width: auto;
    height: auto;    
}

#rcorners3 {
    border-radius: 25px;
    border: 2px solid #317eac;
    padding: 20px; 
    width: auto;
    height: auto;    
}
 </style>

 

 

 